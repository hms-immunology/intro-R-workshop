---
title: "Introduction to Statistics with R"
author: "R Workshop"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    theme: united
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Welcome to Statistics in R!

Have you ever wondered:
- Is this medical treatment actually effective?
- Are these two groups really different from each other?
- How can we be confident in our research findings?

Statistics helps us answer these questions! In this tutorial, we'll learn:
1. How to understand and work with data distributions
2. How to make decisions using statistical tests
3. How to visualize and interpret our results

Don't worry if you're new to statistics - we'll explain everything step by step!

## Part 1: Understanding Data Distributions

### What is a Distribution?

Think of a distribution as a way to see the "shape" of our data. For example:
- Heights in a population
- Test scores in a class
- Gene expression levels in different samples

The most common distribution we see in nature is the "normal" or "bell-shaped" distribution.

### The Normal Distribution: A Bell-Shaped Curve

Let's create and visualize a normal distribution. Imagine we're measuring heights of 1000 people:

```{r normal-dist}
# Generate some example height data (in centimeters)
set.seed(123)  # This makes our random numbers reproducible
heights <- rnorm(1000, mean = 170, sd = 10)  # Mean height 170cm, SD 10cm

# Create a histogram with density scaling
hist(heights, 
     breaks = 30,
     main = "Distribution of Heights in Our Population",
     xlab = "Height (cm)",
     ylab = "Density",
     col = "lightblue",
     border = "white",
     prob = TRUE)  # Use probability scaling for density curves

# Add theoretical normal density curve
curve(dnorm(x, mean = mean(heights), sd = sd(heights)), 
      col = "red", 
      lwd = 2,
      add = TRUE)

# Add empirical density curve
lines(density(heights), 
      col = "darkblue", 
      lwd = 2,
      lty = 2)

# Add legend
legend("topright", 
       legend = c("Theoretical Normal", "Empirical Density"),
       col = c("red", "darkblue"),
       lwd = 2,
       lty = c(1, 2))

# Add some basic statistics
cat("\nSummary of Heights:")
cat("\nAverage Height:", round(mean(heights), 1), "cm")
cat("\nStandard Deviation:", round(sd(heights), 1), "cm")
cat("\nMinimum:", round(min(heights), 1), "cm")
cat("\nMaximum:", round(max(heights), 1), "cm")
```

What can we learn from this plot?
- Most people have heights near the average (170 cm)
- Fewer people have very tall or very short heights
- The red curve shows the "ideal" bell shape

### How Different Factors Affect Our Distribution

Let's look at how changing our measurements affects the shape. Imagine measuring heights in different populations:

```{r distribution-comparison}
# Create three different height distributions
child_heights <- rnorm(1000, mean = 120, sd = 10)    # Children
adult_heights <- rnorm(1000, mean = 170, sd = 10)    # Adults
variable_heights <- rnorm(1000, mean = 170, sd = 20) # More variable group

# Plot them together
plot(density(child_heights), 
     main = "Comparing Height Distributions",
     xlab = "Height (cm)",
     ylab = "Frequency",
     col = "blue",
     lwd = 2,
     xlim = c(80, 220))
lines(density(adult_heights), col = "red", lwd = 2)
lines(density(variable_heights), col = "green", lwd = 2)

# Add a legend
legend("topright", 
       legend = c("Children (mean=120cm)", 
                 "Adults (mean=170cm)", 
                 "Variable Group (more spread out)"),
       col = c("blue", "red", "green"),
       lwd = 2)
```

What do we notice?
- The blue curve (children) is centered at a lower height
- The red curve (adults) is centered at a higher height
- The green curve shows what happens when there's more variability

## Part 2: Making Decisions with Data

### The Basic Question: Are Two Groups Different?

Imagine we're testing a new treatment and want to know if it works. We have:
- A control group (standard treatment)
- A treatment group (new treatment)

Let's look at some example data:

```{r group-comparison}
# Create example data
control <- rnorm(30, mean = 100, sd = 15)    # Control group measurements
treatment <- rnorm(30, mean = 115, sd = 15)   # Treatment group measurements

# Put data in a nice format
experiment_data <- data.frame(
  value = c(control, treatment),
  group = rep(c("Control", "Treatment"), each = 30)
)

# Create a clear visualization
boxplot(value ~ group, data = experiment_data,
        main = "Comparing Control vs Treatment",
        ylab = "Measurement Value",
        col = c("lightblue", "lightgreen"))

# Add individual points
stripchart(value ~ group, data = experiment_data,
           vertical = TRUE, method = "jitter",
           add = TRUE, pch = 19, col = "darkgray")

# Calculate and display some basic statistics
cat("\nControl Group:")
cat("\n  Average:", round(mean(control), 1))
cat("\n  SD:", round(sd(control), 1))
cat("\n\nTreatment Group:")
cat("\n  Average:", round(mean(treatment), 1))
cat("\n  SD:", round(sd(treatment), 1))
```

What are we seeing?
- Each dot represents one measurement
- The boxes show where most of the data falls
- The horizontal line in each box is the average
- The treatment group seems to have higher values, but is this difference real?

### Testing if the Difference is Real: The t-test

To decide if the difference between groups is "real" (statistically significant), we use a t-test:

```{r simple-ttest}
# Perform a t-test
test_result <- t.test(value ~ group, data = experiment_data)

# Print results in a friendly way
cat("\nResults of our Statistical Test:")
cat("\n--------------------------------")
cat("\nAverage difference between groups:", 
    round(diff(tapply(experiment_data$value, experiment_data$group, mean)), 1))
cat("\nIs this difference significant?", 
    ifelse(test_result$p.value < 0.05, "YES!", "No"))
cat("\n(p-value =", round(test_result$p.value, 4), ")")
```

What does this mean?
- If the p-value is less than 0.05, we say the difference is "significant"
- A significant result suggests the difference is probably real, not just by chance
- In this case, the treatment appears to have a real effect!

## Part 3: Common Mistakes to Avoid

1. **Don't jump to conclusions!**
   - A "significant" result doesn't always mean a practically important difference
   - Always look at the actual size of the difference

2. **Check your assumptions**
   - Data should be roughly normal (bell-shaped)
   - Let's check our data:

```{r check-normality}
# Create Q-Q plots to check normality
par(mfrow = c(1, 2))
qqnorm(experiment_data$value[experiment_data$group == "Control"],
       main = "Control Group")
qqline(experiment_data$value[experiment_data$group == "Control"])

qqnorm(experiment_data$value[experiment_data$group == "Treatment"],
       main = "Treatment Group")
qqline(experiment_data$value[experiment_data$group == "Treatment"])
par(mfrow = c(1, 1))
```

If the points follow the line closely, our data is approximately normal!

## Practice Exercises

Try these exercises to test your understanding:

1. Generate your own data for two groups and test if they're different
2. Create a histogram of your data
3. Perform a t-test and interpret the results

## Next Steps

After mastering these basics, you can explore:
- More types of statistical tests
- Working with multiple groups
- Advanced visualization techniques

Remember: Statistics is about telling stories with data. The more you practice, the better you'll get at understanding and telling these stories!

## Additional Resources

1. [UCLA Statistical Methods and Data Analytics](https://stats.oarc.ucla.edu/)
2. [R for Data Science - Statistical Analysis](https://r4ds.had.co.nz/)
3. [Quick R: Basic Statistics](https://www.statmethods.net/stats/index.html)

## Advanced Topics (Optional)

### Understanding Hypothesis Testing in Detail

Hypothesis testing is like being a detective - you start with a assumption (called the null hypothesis) and then look for evidence to disprove it.

1. **The Two Hypotheses**:
   - **Null Hypothesis (H₀)**: Your initial assumption (usually "no effect" or "no difference")
   - **Alternative Hypothesis (H₁)**: What you're looking for evidence to support

```{r hypothesis-advanced}
# Let's simulate a medical trial example
# H₀: New drug has no effect (difference = 0)
# H₁: New drug has an effect (difference ≠ 0)

set.seed(123)  # For reproducible results
placebo <- rnorm(100, mean = 100, sd = 15)    # Placebo group
drug <- rnorm(100, mean = 110, sd = 15)       # Drug group

# Visualize both groups
par(mfrow = c(1, 2))
hist(placebo, main = "Placebo Group", xlab = "Response", col = "lightblue")
hist(drug, main = "Drug Group", xlab = "Response", col = "lightgreen")
par(mfrow = c(1, 1))
```

### Understanding P-values Better

The p-value is often misunderstood. Think of it as the probability of seeing your results (or something more extreme) if there really was no effect.

```{r pvalue-detailed}
# Function to demonstrate p-value concept
demonstrate_pvalue <- function(n_trials = 1000) {
  # Store our results
  differences <- numeric(n_trials)
  
  # Simulate many trials where there is NO real effect
  for(i in 1:n_trials) {
    group1 <- rnorm(30, mean = 100, sd = 15)
    group2 <- rnorm(30, mean = 100, sd = 15)  # Same mean = no effect
    differences[i] <- mean(group2) - mean(group1)
  }
  
  # Plot the distribution of differences
  hist(differences, 
       main = "Distribution of Differences Under No Effect",
       xlab = "Difference Between Groups",
       col = "lightblue",
       breaks = 30)
  
  # Add vertical lines for our observed difference
  observed_diff <- mean(drug) - mean(placebo)
  abline(v = observed_diff, col = "red", lwd = 2)
  abline(v = -observed_diff, col = "red", lwd = 2)
  
  # Add legend
  legend("topright", 
         legend = "Our observed difference",
         col = "red", 
         lwd = 2)
}

# Run the demonstration
demonstrate_pvalue()
```

### Statistical Power and Sample Size

Statistical power is your ability to detect a real effect when it exists. It's like the strength of your microscope - the stronger it is, the better you can see small details.

```{r power-advanced}
# Demonstrate how sample size affects our ability to detect differences
demonstrate_power <- function(sample_sizes = c(10, 30, 100, 300)) {
  # Store results
  power_results <- numeric(length(sample_sizes))
  
  # Test each sample size
  for(i in seq_along(sample_sizes)) {
    n <- sample_sizes[i]
    # Run 1000 trials for each sample size
    significant_results <- replicate(1000, {
      group1 <- rnorm(n, mean = 100, sd = 15)
      group2 <- rnorm(n, mean = 110, sd = 15)  # 10 unit difference
      t.test(group1, group2)$p.value < 0.05
    })
    power_results[i] <- mean(significant_results)
  }
  
  # Create a bar plot
  barplot(power_results,
          names.arg = sample_sizes,
          main = "How Sample Size Affects Power",
          xlab = "Sample Size",
          ylab = "Power (Probability of Detecting Effect)",
          col = "lightblue")
  
  # Add reference line for 80% power
  abline(h = 0.8, col = "red", lty = 2)
  text(length(sample_sizes), 0.85, "Desired Power (80%)", col = "red")
}

# Run the demonstration
demonstrate_power()
```

### Multiple Testing: The Multiple Comparisons Problem

When you perform many tests at once, you increase the chance of finding "significant" results just by chance. It's like buying more lottery tickets - your chance of winning increases even if the odds for each ticket stay the same.

```{r multiple-testing}
# Demonstrate the multiple testing problem
set.seed(123)

# Perform 100 tests where we KNOW there is no real effect
results <- replicate(100, {
  group1 <- rnorm(30, mean = 100, sd = 15)
  group2 <- rnorm(30, mean = 100, sd = 15)  # Same mean = no real effect
  t.test(group1, group2)$p.value
})

# How many are "significant" by chance?
cat("Number of 'significant' results (p < 0.05):", sum(results < 0.05))
cat("\nExpected number by chance:", 100 * 0.05)

# Plot the p-values
hist(results,
     main = "Distribution of P-values\nWhen There Is No Real Effect",
     xlab = "P-value",
     col = "lightblue",
     breaks = 20)
abline(v = 0.05, col = "red", lwd = 2)
text(0.1, 10, "Significance\nThreshold", col = "red")
```

### When to Use These Advanced Concepts

1. **Power Analysis**: Use when planning a study to determine how many samples you need
2. **Multiple Testing Corrections**: Use when performing many tests at once (like in genomics)
3. **Detailed Hypothesis Testing**: Use when you need to be very precise about your conclusions

Remember: These advanced topics are tools in your statistical toolbox. You don't need them for every analysis, but they're good to understand for more complex situations. 